<config>
	<hdfs local-base-dir="/data/appdatas/phoenix/bucket"
		server-uri="hdfs://192.168.213.247/user/workcron/phoenix"
		hdfs-upload-interval="300000">
		<properties>
			<property name="hadoop.security.authentication" value="kerberos" />
			<property name="dfs.namenode.kerberos.principal" value="hadoop/cosmos03.beta@DIANPING.COM" />
			<property name="dfs.cat.kerberos.principal" value="workcron@DIANPING.COM" />
			<property name="dfs.cat.keytab.file" value="/data/appdatas/workcron/workcron.keytab" />
			<property name="java.security.krb5.realm" value="DIANPING.COM" />
			<property name="java.security.krb5.kdc" value="dev80.hadoop" />
		</properties>
	</hdfs>
	<event-expire-time>30000</event-expire-time>
	<retry-queue-clean-interval>1000</retry-queue-clean-interval>
	<retry-queue-safe-length>1000</retry-queue-safe-length>
	<max-l1-cache-size>100000</max-l1-cache-size>
	<max-retry-cache-size>10000</max-retry-cache-size>
	<max-l2-cache-size>1000</max-l2-cache-size>
	<record-file-timespan>300000</record-file-timespan>
	<record-file-write-queue-size>100000</record-file-write-queue-size>
	<record-file-tmp-dir>target/record-tmp/</record-file-tmp-dir>
	<record-file-target-dir>target/record-done/</record-file-target-dir>
	<record-file-write-queue-scan-interval>1000</record-file-write-queue-scan-interval>
	<record-file-write-stream-close-scan-interval>5000</record-file-write-stream-close-scan-interval>
	<record-file-write-stream-multiply>3</record-file-write-stream-multiply>
	<handler-task-threads>2</handler-task-threads>
	<handler-task-queue-capacity>10000</handler-task-queue-capacity>
	<server-list-update-url>http://192.168.22.71/phoenix.txt</server-list-update-url>
</config>